<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>pipeline | 主页</title>
    <link>https://chenfei-ye.github.io/zh/tag/pipeline/</link>
      <atom:link href="https://chenfei-ye.github.io/zh/tag/pipeline/index.xml" rel="self" type="application/rss+xml" />
    <description>pipeline</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>zh-Hans</language><lastBuildDate>Thu, 31 Aug 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://chenfei-ye.github.io/media/icon_hu80a2544e7b046e76dea23103488b26f7_18404_512x512_fill_lanczos_center_3.png</url>
      <title>pipeline</title>
      <link>https://chenfei-ye.github.io/zh/tag/pipeline/</link>
    </image>
    
    <item>
      <title>rs-fMRI预处理完整流程</title>
      <link>https://chenfei-ye.github.io/zh/post/202308_fmripost/</link>
      <pubDate>Thu, 31 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://chenfei-ye.github.io/zh/post/202308_fmripost/</guid>
      <description>&lt;h1 id=&#34;bids-fmripost&#34;&gt;BIDS-fMRIpost&lt;/h1&gt;
&lt;p&gt;&lt;code&gt;BIDS-fmripost&lt;/code&gt;是基于&lt;a href=&#34;https://fmriprep.org/en/stable/installation.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fMRIPrep&lt;/a&gt;  的后处理分析流程，基于&lt;a href=&#34;https://nilearn.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nilearn&lt;/a&gt;开发。分析功能包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;协变量回归（confound regression）&lt;/li&gt;
&lt;li&gt;空间平滑（spatial smoothing）&lt;/li&gt;
&lt;li&gt;基于脑区的BOLD信号提取 (BOLD signal extraction)&lt;/li&gt;
&lt;li&gt;功能连接网络计算 （FC network）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;目前主要用于静息态功能磁共振影像数据的脑网络分析。该脚本的输入数据需符合&lt;a href=&#34;https://bids.neuroimaging.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BIDS格式&lt;/a&gt;，输入模态需包括3D-T1w和fMRI。目前支持的图谱包括：  &lt;code&gt;AAL1_MNI&lt;/code&gt;,  &lt;code&gt;AAL2_MNI&lt;/code&gt;,  &lt;code&gt;AAL3_MNI&lt;/code&gt;,  &lt;code&gt;desikan_T1w&lt;/code&gt;,  &lt;code&gt;destrieux_T1w&lt;/code&gt;,  &lt;code&gt;hcpmmp_T1w&lt;/code&gt;  ,  &lt;code&gt;schaefer100_MNI&lt;/code&gt;,  &lt;code&gt;schaefer200_MNI&lt;/code&gt;  ,  &lt;code&gt;schaefer400_MNI&lt;/code&gt;,  &lt;code&gt;schaefer1000_MNI&lt;/code&gt;,  &lt;code&gt;PD25_MNI&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;具体使用说明&lt;a href=&#34;https://github.com/chenfei-ye/BIDS-fMRIpost/blob/main/resources/README_Chs.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;点此&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>vtk文件转mesh格式</title>
      <link>https://chenfei-ye.github.io/zh/post/202308_vtk2mesh/</link>
      <pubDate>Thu, 31 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://chenfei-ye.github.io/zh/post/202308_vtk2mesh/</guid>
      <description>&lt;h1 id=&#34;vtk文件转mesh格式&#34;&gt;vtk文件转mesh格式&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;安装gmsh的binary文件 &lt;a href=&#34;https://gmsh.info/bin/Linux/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gmsh.info/bin/Linux/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;转档&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# 方式1 
gmsh -3 -o .msh 
# 方式2 
gmsh -3 -save
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;本地安装gmsh GUI软件打开验证&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;其实也可以用&lt;code&gt;gmsh GUI&lt;/code&gt;直接转档&lt;/p&gt;
&lt;h3 id=&#34;注意容器系统环境可能需要安装libxcursor1才能支持gmsh&#34;&gt;注意：容器系统环境可能需要安装libxcursor1才能支持gmsh&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apt install libxcursor1
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>关于最近一些MRI-T1w结构像处理pipeline的测试</title>
      <link>https://chenfei-ye.github.io/zh/post/202303_parcellate/</link>
      <pubDate>Sun, 12 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://chenfei-ye.github.io/zh/post/202303_parcellate/</guid>
      <description>&lt;h2 id=&#34;fastsurfer---全脑分割&#34;&gt;FastSurfer -&amp;gt; 全脑分割&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://deep-mi.org/research/fastsurfer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://deep-mi.org/research/fastsurfer/&lt;/a&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://deep-mi.org/static/img/research/fastsurfer/01_teaser_white.png&#34; alt=&#34;!&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

FastSurfer is a fast and &lt;a href=&#34;https://deep-mi.org/research/fastsurfer/#proof-of-concept&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;extensively validated&lt;/a&gt; deep-learning pipeline for the fully automated processing of structural human brain MRIs. As such, it provides FreeSurfer conform outputs, enables scalable big-data analysis and time-critical clinical applications such as structure localization during image acquisition or extraction of quantitative measures.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# 完整分割（GPU） 
docker run --gpus all -v /data/mg_data/mnt/bl_test/input:/data -v /data/mg_data/mnt/bl_test/fastsf_output:/output -v /data/freesurfer:/fs_license --rm deepmi/fastsurfer:gpu-v1.1.1 --fs_license /fs_license/freesurfer_license.txt --t1 /data/t1.nii --sid subject2 --sd /output --parallel 

# DKT Lookup table 
31和63作为脉络丛会被分割出来，应该当做LV，对应4和43 
77作为WMH，没有区分左右，不便合并
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;synthseg---全脑分割计算icv&#34;&gt;SynthSeg -&amp;gt; 全脑分割，计算ICV&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/BBillot/SynthSeg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/BBillot/SynthSeg&lt;/a&gt;
&lt;a href=&#34;https://surfer.nmr.mgh.harvard.edu/fswiki/SynthSeg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://surfer.nmr.mgh.harvard.edu/fswiki/SynthSeg&lt;/a&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://surfer.nmr.mgh.harvard.edu/fswiki/SynthSeg?action=AttachFile&amp;amp;do=get&amp;amp;target=robust2.png&#34; alt=&#34;!&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;code&gt;SynthSeg&lt;/code&gt; 可不依赖GPU，实测CPU多线程计算约2分钟内完成&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# 分割 （完整） 
mri_synthseg --i /dataio/t1.nii --parc --robust --vol /dataio/vol.csv --qc /dataio/qc.csv --threads 172 --o /dataio/t1_seg.nii.gz 
# 注意 --resample如果输入本身是1mm iso，则不会生成resample图像
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;synthstrip---剥头皮&#34;&gt;SynthStrip -&amp;gt; 剥头皮&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://surfer.nmr.mgh.harvard.edu/docs/synthstrip/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://surfer.nmr.mgh.harvard.edu/docs/synthstrip/&lt;/a&gt;
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://surfer.nmr.mgh.harvard.edu/docs/synthstrip/resources/SynthStripExamples.png&#34; alt=&#34;!&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

SynthStrip is a skull-stripping tool that extracts brain signal from a landscape of image types, ranging across imaging modality, contrast, resolution, and subject population. It leverages a deep learning strategy that synthesizes arbitrary training images from segmentation maps to optimize a robust model agnostic to acquisition specifics.
&lt;a href=&#34;https://doi.org/10.1016/j.neuroimage.2022.119474&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1016/j.neuroimage.2022.119474&lt;/a&gt;&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# 自动去头皮（多模态泛化性优秀），1分钟以内 
mri_synthstrip -i /dataio/t1.nii -o /dataio/t1_bet.nii -m /dataio/t1_mask.nii
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;sbtiv--计算icv&#34;&gt;sbTIV-&amp;gt; 计算ICV&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://freesurfer.net/fswiki/Samseg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://freesurfer.net/fswiki/Samseg&lt;/a&gt;
&lt;a href=&#34;https://freesurfer.net/fswiki/sbTIV&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://freesurfer.net/fswiki/sbTIV&lt;/a&gt;
Total intracranial volume (TIV/ICV) is an important covariate for volumetric analyses of the brain and brain regions. It is commonly used to correct for head size variation (i.e., &amp;rsquo;normalize&amp;rsquo; hippocampal volume size). The gold-standard method is manual delineation of T2 scans. Freesurfer currently provides the eTIV measure, described &lt;a href=&#34;https://freesurfer.net/fswiki/eTIV&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. It has been shown to be a robust covariate.
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://freesurfer.net/fswiki/Samseg?action=AttachFile&amp;amp;do=get&amp;amp;target=3D_small.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;## 计算sbTIV (segmentation-based TIV)，大约5-10分钟
run_samseg --input /home/username/data/t1.nii --output /home/username/data/samsegOutput/ --threads 8
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;关于freesurfer图谱&#34;&gt;关于FreeSurfer图谱&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;DK图谱lookuptable &lt;a href=&#34;https://surfer.nmr.mgh.harvard.edu/fswiki/FsTutorial/AnatomicalROI/FreeSurferColorLUT&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://surfer.nmr.mgh.harvard.edu/fswiki/FsTutorial/AnatomicalROI/FreeSurferColorLUT&lt;/a&gt; &lt;a href=&#34;https://www.cis.jhu.edu/~parky/MRN/Desikan%20Region%20Labels%20and%20Descriptions.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.cis.jhu.edu/~parky/MRN/Desikan%20Region%20Labels%20and%20Descriptions.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DKT 图谱： &lt;a href=&#34;https://mindboggle.readthedocs.io/en/latest/labels.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://mindboggle.readthedocs.io/en/latest/labels.html&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;HCPMMP图谱 ： &lt;a href=&#34;https://neuroimaging-core-docs.readthedocs.io/en/latest/pages/atlases.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://neuroimaging-core-docs.readthedocs.io/en/latest/pages/atlases.html&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>python进程传参踩坑：为何单变量参数被识别为多变量</title>
      <link>https://chenfei-ye.github.io/zh/post/202206_python_tuple/</link>
      <pubDate>Wed, 15 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://chenfei-ye.github.io/zh/post/202206_python_tuple/</guid>
      <description>&lt;h3 id=&#34;背景&#34;&gt;背景：&lt;/h3&gt;
&lt;p&gt;新建一个脚本进程，需要如下传参：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;fastcsr_input = [&amp;#39;--t1&amp;#39;,input_nu_image, &amp;#39;--L4&amp;#39;, t1_label_L4]
p_fastcsr = multiprocessing.Process(target=FastCSR_pipeline.main, args=(fastcsr_input))
p_fastcsr.start()
p_fastcsr.join()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;脚本&lt;code&gt;FastCSR_pipeline.main&lt;/code&gt;函数只接受一个输入变量，但运行时报错：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;python TypeError: write() takes exactly 1 argument (but 4 were given)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;方案&#34;&gt;方案：&lt;/h3&gt;
&lt;p&gt;折腾半天找到&lt;a href=&#34;https://stackoverflow.com/questions/1559125/string-arguments-in-python-multiprocessing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;原因&lt;/a&gt;，&lt;code&gt;multiprocessing.Process&lt;/code&gt;中&lt;code&gt;args&lt;/code&gt;变量是&lt;code&gt;tuple&lt;/code&gt;类型，&lt;code&gt;tuple&lt;/code&gt;中若只有一个对象，需要在后面加一个逗号。。&lt;/p&gt;
&lt;p&gt;所以修改代码为：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;fastcsr_input = [&amp;#39;--t1&amp;#39;,input_nu_image, &amp;#39;--L4&amp;#39;, t1_label_L4]
p_fastcsr = multiprocessing.Process(target=FastCSR_pipeline.main, args=(fastcsr_input,))
p_fastcsr.start()
p_fastcsr.join()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;即可顺利运行&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CUDA版本、显卡算力、Pytorch对应关系</title>
      <link>https://chenfei-ye.github.io/zh/post/202206_cuda_pytorch/</link>
      <pubDate>Sat, 11 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://chenfei-ye.github.io/zh/post/202206_cuda_pytorch/</guid>
      <description>&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;
&lt;p&gt;试用合作医院提供的堡垒机服务器，自带的NVIDIA-A6000显卡不支持结构像自动分割的pipeline镜像，运行时报错capability sm_86 is not compatible。发现是由于显卡的架构比较新，旧版pytorch库不支持。同时根据输出可以看到 The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70 sm_75当前pytorch只能支持上面几种（显卡算力）架构。&lt;/p&gt;
&lt;h2 id=&#34;解决方案&#34;&gt;解决方案&lt;/h2&gt;
&lt;p&gt;把基础镜像直接更新到&lt;code&gt;pytorch/pytorch:1.7.0-cuda11.0-cudnn8-runtime&lt;/code&gt;，解决。
NOTE: CUDA10.x最高支持算力7.x; CUDA11.0最高支持算力8.x&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://imgpp.com/s1/2022/06/12/cuda_version.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://imgpp.com/s1/2022/06/12/torch_version.png&#34; alt=&#34;png&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
